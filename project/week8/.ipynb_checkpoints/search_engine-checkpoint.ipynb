{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6af865a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter an English query: Hello My name is Subin Ahn\n",
      "Rank\tIndex\tScore\tSentence\n",
      "1\t679\t0.42857142857142855\tMy name is Mike.\n",
      "2\t526\t0.25\tBob is my brother.\n",
      "3\t538\t0.25\tMy hobby is traveling.\n",
      "4\t453\t0.2222222222222222\tMy mother is sketching them.\n",
      "5\t241\t0.2\tMy father is running with So-ra.\n",
      "6\t336\t0.2\tMy family is at the park.\n",
      "7\t212\t0.18181818181818182\tMy sister Betty is waiting for me.\n",
      "8\t505\t0.16666666666666666\tMy little sister Annie is five years old.\n",
      "9\t610\t0.14285714285714285\tI would raise my voice and yell, \"LUNCH IS READY!\"\n",
      "10\t190\t0.125\tIt is Sunday.\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(sentence):\n",
    "    # Strip leading and trailing spaces, convert to lowercase, and split into tokens using space as a delimiter.\n",
    "    preprocessed_sentence = sentence.strip().lower().split(\" \")\n",
    "    return preprocessed_sentence  # Return the preprocessed token list.\n",
    "\n",
    "# File indexing function.\n",
    "def indexing(file_name):\n",
    "    # Tokenize each line in the given file and return the results as a list.\n",
    "\n",
    "    file_tokens_pairs = []  # Initialize a list to store pairs of lines and their tokens.\n",
    "    lines = open(file_name, \"r\", encoding=\"utf8\").readlines()  # Read all lines from the file.\n",
    "\n",
    "    for line in lines:\n",
    "        tokens = preprocess(line)  # Preprocess each line to obtain tokens.\n",
    "        file_tokens_pairs.append(tokens)  # Add the tokenized line to the list.\n",
    "        print(tokens)  # Print the tokens (for debugging).\n",
    "    \n",
    "    return file_tokens_pairs  # Return the list of tokenized lines.\n",
    "\n",
    "# Similarity calculation function.\n",
    "def calc_similarity(preprocessed_query, preprocessed_sentences):\n",
    "    score_dict = {}  # Initialize a dictionary to store similarity scores.\n",
    "\n",
    "    # Lowercase the query for case-insensitive matching.\n",
    "    query_str = ' '.join(preprocessed_query).lower()\n",
    "    preprocessed_query = set(preprocess(query_str))  # Convert the query to a set of preprocessed tokens.\n",
    "\n",
    "    for i, sentence in enumerate(preprocessed_sentences):\n",
    "        # Lowercase the sentence for case-insensitive matching.\n",
    "        sentence_str = ' '.join(sentence).lower()\n",
    "        preprocessed_sentence = set(preprocess(sentence_str))  # Convert the sentence to a set of preprocessed tokens.\n",
    "\n",
    "        # Calculate similarity as the ratio of common tokens to all tokens.\n",
    "        all_tokens = preprocessed_query | preprocessed_sentence  # Union of tokens.\n",
    "        same_tokens = preprocessed_query & preprocessed_sentence  # Intersection of tokens.\n",
    "        similarity = len(same_tokens) / len(all_tokens)\n",
    "\n",
    "        score_dict[i] = similarity  # Store the similarity score for this sentence.\n",
    "\n",
    "    return score_dict  # Return the dictionary of similarity scores.\n",
    "\n",
    "# 1. Indexing\n",
    "## Source: https://github.com/jungyeul/korean-parallel-corpora\n",
    "file_name = \"jhe-koen-dev.en\"\n",
    "lines = open(file_name, \"r\", encoding=\"utf8\").readlines()\n",
    "file_tokens_pairs = []  # Initialize a list to store pairs of lines and their tokens.\n",
    "for line in lines:\n",
    "    tokens = line.strip().split(\" \")  # Split each line into tokens.\n",
    "    file_tokens_pairs.append(tokens)  # Add the tokenized line to the list.\n",
    "\n",
    "# 2. Input the query\n",
    "query = input(\"Enter an English query: \")  # Prompt the user to enter an English query.\n",
    "preprocessed_query = query.strip().split(\" \")  # Preprocess the query and tokenize it.\n",
    "query_token_set = set(preprocessed_query)  # Convert the query to a set of preprocessed tokens.\n",
    "\n",
    "# 3. Calculate similarities based on a common token set\n",
    "score_dict = calc_similarity(preprocessed_query, file_tokens_pairs)  # Calculate similarity scores.\n",
    "\n",
    "# 4. Sort the similarity list\n",
    "sorted_score_list = sorted(score_dict.items(), key=operator.itemgetter(1), reverse=True)  # Sort scores in descending order.\n",
    "\n",
    "# 5. Print the result\n",
    "if sorted_score_list[0][1] == 0.0:\n",
    "    print(\"There is no similar sentence.\")\n",
    "else:\n",
    "    print(\"Rank\", \"Index\", \"Score\", \"Sentence\", sep=\"\\t\")  # Print table headers.\n",
    "    rank = 1\n",
    "    for i, score in sorted_score_list:\n",
    "        print(rank, i, score, ' '.join(file_tokens_pairs[i]), sep=\"\\t\")  # Print the ranked results.\n",
    "        if rank == 10:\n",
    "            break\n",
    "        rank = rank + 1  # Increment the rank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ac2ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
